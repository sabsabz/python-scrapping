{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b9dcdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20132\\4187912284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0m_name_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"_main_\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasicConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "def get_bs(session, url):\n",
    "    \"\"\"Makes a GET requests using the given Session object\n",
    "    and returns a BeautifulSoup object.\n",
    "    \"\"\"\n",
    "    r = None\n",
    "    while True:\n",
    "        r = session.get(url)\n",
    "        if r.ok:\n",
    "            break\n",
    "    return BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "\n",
    "def make_login(session, base_url, credentials):\n",
    "    \"\"\"Returns a Session object logged in with credentials.\n",
    "    \"\"\"\n",
    "    login_form_url = '/login/device-based/regular/login/?refsrc=https%3A'\\\n",
    "        '%2F%2Fmobile.facebook.com%2Flogin%2Fdevice-based%2Fedit-user%2F&lwv=100'\n",
    "\n",
    "    params = {'email':credentials['email'], 'pass':credentials['pass']}\n",
    "\n",
    "    while True:\n",
    "        time.sleep(3)\n",
    "        logged_request = session.post(base_url+login_form_url, data=params)\n",
    "        \n",
    "        if logged_request.ok:\n",
    "            logging.info('[*] Logged in.')\n",
    "            break\n",
    "\n",
    "\n",
    "def crawl_profile(session, base_url, profile_url, post_limit):\n",
    "    \"\"\"Goes to profile URL, crawls it and extracts posts URLs.\n",
    "    \"\"\"\n",
    "    profile_bs = get_bs(session, profile_url)\n",
    "    n_scraped_posts = 0\n",
    "    scraped_posts = list()\n",
    "    posts_id = None\n",
    "\n",
    "    while n_scraped_posts < post_limit:\n",
    "        try:\n",
    "            posts_id = 'recent'\n",
    "            posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "        except Exception:\n",
    "            posts_id = 'structured_composer_async_container'\n",
    "            posts = profile_bs.find('div', id=posts_id).div.div.contents\n",
    "\n",
    "        posts_urls = [a['href'] for a in profile_bs.find_all('a', text='Full Story')] \n",
    "\n",
    "        for post_url in posts_urls:\n",
    "            # print(post_url)\n",
    "            try:\n",
    "                post_data = scrape_post(session, base_url, post_url)\n",
    "                scraped_posts.append(post_data)\n",
    "            except Exception as e:\n",
    "                logging.info('Error: {}'.format(e))\n",
    "            n_scraped_posts += 1\n",
    "            if posts_completed(scraped_posts, post_limit):\n",
    "                break\n",
    "        \n",
    "        show_more_posts_url = None\n",
    "        if not posts_completed(scraped_posts, post_limit):\n",
    "            show_more_posts_url = profile_bs.find('div', id=posts_id).next_sibling.a['href']\n",
    "            profile_bs = get_bs(session, base_url+show_more_posts_url)\n",
    "            time.sleep(3)\n",
    "        else:\n",
    "            break\n",
    "    return scraped_posts\n",
    "\n",
    "def posts_completed(scraped_posts, limit):\n",
    "    \"\"\"Returns true if the amount of posts scraped from\n",
    "    profile has reached its limit.\n",
    "    \"\"\"\n",
    "    if len(scraped_posts) == limit:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def scrape_post(session, base_url, post_url):\n",
    "    \"\"\"Goes to post URL and extracts post data.\n",
    "    \"\"\"\n",
    "    post_data = OrderedDict()\n",
    "\n",
    "    post_bs = get_bs(session, base_url+post_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Here we populate the OrderedDict object\n",
    "    post_data['url'] = post_url\n",
    "\n",
    "    try:\n",
    "        post_text_element = post_bs.find('div', id='u_0_0').div\n",
    "        string_groups = [p.strings for p in post_text_element.find_all('p')]\n",
    "        strings = [repr(string) for group in string_groups for string in group]\n",
    "        post_data['text'] = strings\n",
    "    except Exception:\n",
    "        post_data['text'] = []\n",
    "    \n",
    "    try:\n",
    "        post_data['media_url'] = post_bs.find('div', id='u_0_0').find('a')['href']\n",
    "    except Exception:\n",
    "        post_data['media_url'] = ''\n",
    "    \n",
    "\n",
    "    try:\n",
    "        post_data['comments'] = extract_comments(session, base_url, post_bs, post_url)\n",
    "    except Exception:\n",
    "        post_data['comments'] = []\n",
    "    \n",
    "    return dict(post_data)\n",
    "\n",
    "\n",
    "def extract_comments(session, base_url, post_bs, post_url):\n",
    "    \"\"\"Extracts all coments from post\n",
    "    \"\"\"\n",
    "    comments = list()\n",
    "    show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))['href']\n",
    "    first_comment_page = True\n",
    "\n",
    "    logging.info('Scraping comments from {}'.format(post_url))\n",
    "    while True:\n",
    "\n",
    "        logging.info('[!] Scraping comments.')\n",
    "        time.sleep(3)\n",
    "        if first_comment_page:\n",
    "            first_comment_page = False\n",
    "        else:\n",
    "            post_bs = get_bs(session, base_url+show_more_url)\n",
    "            time.sleep(3)\n",
    "        \n",
    "        try:\n",
    "            comments_elements = post_bs.find('div', id=re.compile('composer')).next_sibling\\\n",
    "                .find_all('div', id=re.compile('^\\d+'))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if len(comments_elements) != 0:\n",
    "            logging.info('[!] There are comments.')\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "        for comment in comments_elements:\n",
    "            comment_data = OrderedDict()\n",
    "            comment_data['text'] = list()\n",
    "            try:\n",
    "                comment_strings = comment.find('h3').next_sibling.strings\n",
    "                for string in comment_strings:\n",
    "                    comment_data['text'].append(string)\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                media = comment.find('h3').next_sibling.next_sibling.children\n",
    "                if media is not None:\n",
    "                    for element in media:\n",
    "                        comment_data['media_url'] = element['src']\n",
    "                else:\n",
    "                    comment_data['media_url'] = ''\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            comment_data['profile_name'] = comment.find('h3').a.string\n",
    "            comment_data['profile_url'] = comment.find('h3').a['href'].split('?')[0]\n",
    "            comments.append(dict(comment_data))\n",
    "        \n",
    "        show_more_url = post_bs.find('a', href=re.compile('/story\\.php\\?story'))\n",
    "        if 'View more' in show_more_url.text:\n",
    "            logging.info('[!] More comments.')\n",
    "            show_more_url = show_more_url['href']\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return comments\n",
    "\n",
    "\n",
    "def json_to_obj(filename):\n",
    "    \"\"\"Extracts dta from JSON file and saves it on Python object\n",
    "    \"\"\"\n",
    "    obj = None\n",
    "    with open(filename) as json_file:\n",
    "        obj = json.loads(json_file.read())\n",
    "    return obj\n",
    "\n",
    "\n",
    "def save_data(data):\n",
    "    \"\"\"Converts data to JSON.\n",
    "    \"\"\"\n",
    "    with open('profile_posts_data.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    base_url = 'https://mobile.facebook.com'\n",
    "    session = requests.session()\n",
    "\n",
    "    # Extracts credentials for the login and all of the profiles URL to scrape\n",
    "    # = json_to_obj('credentials.json')\n",
    "    #profiles_urls = json_to_obj('profiles_urls.json')\n",
    "\n",
    "    #make_login(session, base_url, credentials)\n",
    "\n",
    "    # posts_data = None\n",
    "    # for profile_url in profiles_urls:\n",
    "    #     posts_data = crawl_profile(session, base_url, profile_url, 25)\n",
    "    # logging.info('[!] Scraping finished. Total: {}'.format(len(posts_data)))\n",
    "    # logging.info('[!] Saving.')\n",
    "    # save_data(posts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8fd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
